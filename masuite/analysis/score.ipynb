{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve efficiency: pre-allocate df, use only big score, and calculate the other df after the fact\n",
    "<br>\n",
    "To improve flexibility: Extract columns titles and use those titles for DF columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrame from csv files\n",
    "def df_data(algo, epoch, batch_size, lr):\n",
    "    fname = algo + \"-gridhunter-0_epochs\" + str(epoch) + \"_batch_size\" + str(batch_size) + \"_lr\" + str(lr) + \"_.csv\"\n",
    "    file_name = \"C:/Users/drago/OneDrive - UW/Ratliff Research/gridhunter_data/\"+ fname\n",
    "    simple_data = pd.read_csv(file_name)\n",
    "    return simple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates Score in each epoch, returns dataframe of results total, batch, and epoch scores\n",
    "\n",
    "def score(algos, epochs, batch_sizes, lrs):\n",
    "    big_score_df = df({'Algo': [], 'Epochs': [], 'Batch Size': [], 'Learning Rate': [], 'Epoch': [], 'Return Difference': []})\n",
    "    score_df = df({'Algo': [], 'Epochs': [], 'Batch Size': [], 'Learning Rate': [], 'ag0_score': [], 'ag1_score': []})\n",
    "    batch_df = df({'Algo': [], 'Epochs': [], 'Batch Size': [], 'batch_ag0_score': [], 'batch_ag1_score': []})\n",
    "    epoch_df = df({'Algo': [], 'Epochs': [], 'epoch_ag0_score': [], 'epoch_ag1_score': []})\n",
    "    for algo in algos:\n",
    "        ag0_score = 0\n",
    "        ag1_score = 0\n",
    "        count = 0\n",
    "        for epoch in epochs:\n",
    "            epoch_count = count\n",
    "            epoch_ag0_score = ag0_score\n",
    "            epoch_ag1_score = ag1_score\n",
    "            for batch in batch_sizes:\n",
    "                batch_count = count\n",
    "                batch_ag0_score = ag0_score\n",
    "                batch_ag1_score = ag1_score\n",
    "                for lr in lrs:\n",
    "                    data_df = df_data(algo, epoch, batch, lr)\n",
    "                    data = data_df[['agent0_avg_rets', 'agent1_avg_rets']]\n",
    "                    return_data = df.to_numpy(data)\n",
    "                    trials = len(return_data[:,0])\n",
    "                    lr_ag0_score = ag0_score\n",
    "                    lr_ag1_score = ag1_score\n",
    "                    for idx in np.arange(0,trials):\n",
    "                        return_diff = 0\n",
    "                        count += 1\n",
    "                        if (return_data[idx,0] > return_data[idx,1]):\n",
    "                            ag0_score += 1\n",
    "                        else:\n",
    "                            ag1_score += 1\n",
    "                        return_diff = return_data[idx,0] - return_data[idx,1]\n",
    "                        big_score_df.loc[len(big_score_df.index)] = [algo, epoch, batch, lr, idx, return_diff] \n",
    "                    lr_ag0_score = (ag0_score - lr_ag0_score) / trials\n",
    "                    lr_ag1_score = (ag1_score - lr_ag1_score) / trials\n",
    "                    score_df.loc[len(score_df.index)] = [algo, epoch, batch, lr, lr_ag0_score, lr_ag1_score] \n",
    "                batch_ag0_score = (ag0_score - batch_ag0_score) / (count - batch_count)\n",
    "                batch_ag1_score = (ag1_score - batch_ag1_score) / (count - batch_count)\n",
    "                batch_df.loc[len(batch_df.index)] = [algo, epoch, batch, batch_ag0_score, batch_ag1_score] \n",
    "                # print('Batch Score: algo = '+ algo + ' Batch = ' + str(batch) + ' ag0_score = ' + str(round(batch_ag0_score,5)) +   ' ag1_score = ' + str(round(batch_ag1_score,5)))\n",
    "            epoch_ag0_score = (ag0_score - epoch_ag0_score) / (count - epoch_count)\n",
    "            epoch_ag1_score = (ag1_score - epoch_ag1_score) / (count - epoch_count)\n",
    "            epoch_df.loc[len(epoch_df.index)] = [algo, epoch, epoch_ag0_score, epoch_ag1_score] \n",
    "            # print('Epoch Score: algo = '+ algo + ' Epoch = ' + str(epoch) + ' ag0_score = ' + str(round(epoch_ag0_score,5)) +   ' ag1_score = ' + str(round(epoch_ag1_score,5)))\n",
    "        ag0_score /= count\n",
    "        ag1_score /= count\n",
    "        print('    Total Score: algo = '+ algo + ' ag0_score = ' + str(round(ag0_score,5)) +   ' ag1_score = ' + str(round(ag1_score,5)))\n",
    "        \n",
    "    return big_score_df, score_df, batch_df, epoch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Use\n",
    "algos = ['SimplePG', 'StackPG']\n",
    "epochs = [50, 100, 500]\n",
    "batch_sizes = [100, 500, 1000, 5000]\n",
    "lrs = [0.001, 0.01, 0.1]\n",
    "big_score_df, score_df, batch_df, epoch_df = score(algos, epochs, batch_sizes, lrs)\n",
    "\n",
    "big_score_df.to_pickle(\"C:/Users/drago/OneDrive - UW/Ratliff Research/gridhunter_data/big_score_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
